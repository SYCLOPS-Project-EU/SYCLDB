# SYCLDB

SYCLDB is a library of templated SYCL host and device functions that implement a set of primitives necessary for executing typical analytic SQL SPJA analytical queries. At present SYCLDB supports projections, selections, hash joins, and aggregations as fundamental relational operators.

## Compilation & SSB Data Generation

Both hash join and projection generate own sample input, so you could directly run then as:
```bash
make join
./join
make project
./project
```

The SSB benchmarks require a test dataset to be generated first. This can be done by running the following commands:
```bash
cd test/ssb/dbgen
make
cd ../loader
make
cd ../..
python3 util.py 20 # scale factor 20 ~ 2 GB
```

Then back in the main directory:
```bash
make ssb/qXX # XX being 11, 12, 13, 21, 22, 23, 31, 32, 33, 34, 41, 42, 43
./ssb/qXX
```

## Comments

### Specifying compiler

By default, all benchmarks are compiled with `icpx` (from Intel oneAPI). It is also possible to specify `acpp` and `clang++` as compilers.
```bash
make COMPILER=clang++ join
```

In addition, one could specify own clang installation path such as:
```bash
make PATH_TO_LLVM=/home/ivan/llvm/build/install COMPILER=clang++ join
```

### Execution flags

All benchmarks accept as command line arguments:
* `-p` - number of partitions to split the probe/projection table into;
* `-g` - number of GPUs/devices to use - all devices compute independently on them the build tables, and then each device processes a part of the probe table;
* `-r` - number of repetitions to run the query, where each repetition allocates (and deallocates) new device memory, and then transfers and computes the data;

In addtion, join accepts:
* `-t` - determining whether 1d-non-tiled (0) or tile-based probe (>=1) is used;
* `-d` - size of the build table in number of tuples;
* `-f` - size of the probe table in number of tuples;

Whereas, projection also accepts:
* `-t` - determining whether 1d-non-tiled (0) or tile-based projection (>=1) is used;
* `-n` - size of the table in number of tuples;
* `-s` - determining whether dot-product (0) or sigmoid (>=1) is used

The `ssb/ssb_utils.hpp` expects the data to be in the directory generated by the first step. The macro `SF` defined there has default values for several scale factors - namely 1, 10, 20, 100 - we typically use 100 and shrink the probe macro (`LO_LEN`) as needed.

To match the description for the CIDR submission, we did not laverage exchange, and hence set the number of GPUs/devices as well as the number of partitions to 1.

### Setting other devices

#### x64

Specifying `-g 0` would default to the CPU backend. For cases `-g 1`: SYCLDB identifies the devices on which to run inside of `exchange.hpp` - currently the necessary condition is that the device type is a GPU (i.e. `is_gpu()`, could be changed to `is_cpu()` or `is_accelerator()`, etc.).

In the event, no GPU is found, but there is one present in the system - verify with `sycl-ls` if the device is visible to the runtime. [OneAPI for Nvida GPUs](https://developer.codeplay.com/products/oneapi/nvidia/download/) is required.

#### RISC-V

There are two scenarios to consider:

1. Running on a RISC-V (remote) accelerator - need to establish a client-server connection via a [HAL server provided from the oneapi-construction-kit](https://github.com/codeplaysoftware/oneapi-construction-kit/blob/main/examples/hal_cpu_remote_server/README.md#building-the-client). The only modification needed to run on a RISC-V accelerator is to change the `is_gpu()` condition in `exchange.hpp` to `is_accelerator()`.
2. Running on a RISC-V (local) CPU backend. We could either use again a HAL server, but could also directly target the hardware, avoiding transfers. For the latter, we would get a custom SYCL implementation for the RISC-V architecture. Procedure as:
* DPCPP: Clone [https://github.com/PietroGhg/llvm/pietro/cross_riscv](https://github.com/PietroGhg/llvm/pietro/cross_riscv), and configure with `python $PWD/llvm/buildbot/configure.py --host-target="RISCV" --native_cpu --native_cpu_libclc_targets "riscv64-unknown-linux-gnu"`. But then, this could target the architecture only natively (which lacks support for many features), so we need OCK to recognize the CPU as an OpenCL device. But OCK requires SPIR-V tools and LLVM (check the supported versions by OCK).
* SPIR-V:
```bash
git clone https://github.com/KhronosGroup/SPIRV-Tools
cd SPIRV-Tools
cmake  -DCMAKE_INSTALL_PREFIX=$PWD/build/install -G Ninja -S . -B build
ninja -C build install
```
* LLVM:
```bash
cd ~/DPCPP
git clone  https://github.com/llvm/llvm-project -b release/18.x
cd llvm-project
cmake -S llvm -B build -G Ninja -DLLVM_ENABLE_PROJECTS="clang" -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_ASSERTIONS=On -DLLVM_ENABLE_ZLIB=Off -DLLVM_ENABLE_ZSTD=Off -DCMAKE_INSTALL_PREFIX=./build/install
ninja -C build install
```
* Finally, once SPIR-V and LLVM are installed, we can install OCK:
```bash
export VULKAN_SDK=~/DPCPP/SPIRV-Tools/build
export LLVMInstall=~/DPCPP/llvm-project/build/install

cd ~/DPCPP
git clone https://github.com/codeplaysoftware/oneapi-construction-kit
cd oneapi-construction-kit
cmake -B build -S . -DCA_ENABLE_API=cl -GNinja -DCA_CL_ENABLE_ICD_LOADER=ON -DOCL_EXTENSION_cl_khr_command_buffer=ON -DOCL_EXTENSION_cl_khr_command_buffer_mutable_dispatch=ON -DOCL_EXTENSION_cl_khr_extended_async_copies=ON -DSpirvTools_spirv-as_EXECUTABLE=$VULKAN_SDK/tools/spirv-as -DCMAKE_BUILD_TYPE=Release -DCA_ENABLE_DOCUMENTATION=Off -DCMAKE_INSTALL_PREFIX=$PWD/build/install -DCA_LLVM_INSTALL_DIR=$LLVMInstall
ninja -C build install
```
* The minimal set of `SYCLONDEVICE` flags in the Makefile to run on RISC-V CPU becomes: `-fsycl -fsycl-targets=spir64 --target=riscv64-redhat-linux -fsycl-llc-options=-mattr=+m,+f,+a,+d` (instead of `-fsycl-targets=nvptx64-nvidia-cuda,spir64`). And then, one would need to set the `is_cpu()` condition in `exchange.hpp` to target the RISC-V CPU backend.